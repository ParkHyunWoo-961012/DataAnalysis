{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"오토인코더활용이상탐지.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNNx9VzibSUISHiylFFSy8p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0vwRHpR2Vg3","executionInfo":{"status":"ok","timestamp":1657944098202,"user_tz":-540,"elapsed":4655,"user":{"displayName":"박현우","userId":"12791002538251337460"}},"outputId":"147c8e55-a1de-4dff-e262-6d841f53fa6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"]}],"source":["# 데이터 가져오기\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","# One_Class Support Vector Machine , AutoEncoder, Isololation Forest \n","\n","path = \"/content/gdrive/Shareddrives/DACON_fds/source/fds/\"\n","# 샘플 결과 저장 \n","submission_path = \"/content/gdrive/Shareddrives/DACON_fds/source/submission/\"\n","\n","train = pd.read_csv(path + \"train.csv\")\n","test = pd.read_csv(path +\"test.csv\")\n","val = pd.read_csv(path +\"val.csv\")\n","sample = pd.read_csv(path +\"sample_submission.csv\")\n"]},{"cell_type":"code","source":[""],"metadata":{"id":"BRJE6qACKXp1","executionInfo":{"status":"ok","timestamp":1657944100054,"user_tz":-540,"elapsed":270,"user":{"displayName":"박현우","userId":"12791002538251337460"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from tensorflow.keras.layers import Input, Dense, Dropout\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","train = pd.read_csv(\"open/train.csv\")\n","val = pd.read_csv(\"open/val.csv\")\n","\n","train.drop(columns = {\"ID\"},inplace=True)\n","val.drop(columns = {\"ID\"},inplace=True)\n","\n","test = val[val.columns[:-1]]\n","\n","nFeature=train.shape[1]\n","inputX = Input(batch_shape=(None,nFeature))\n","\n","Xencode = Dense(12, activation='relu')(inputX)\n","Xdecode = Dense(nFeature, activation='linear')(Xencode)\n","\n","model = Model(inputX, Xdecode)\n","model.compile(loss='mse', optimizer=Adam(lr=0.001))\n","\n","h = model.fit(train,train,epochs=300, batch_size=1000, shuffle=True)\n","yhat=model.predict(test)\n","\n","dist_list=[]\n","for i in range(len(yhat)):\n","    dist = cosine_similarity([test.iloc[0].values],[yhat[0]])\n","    dist_list.append(dist)\n","\n","dist_list2 = dist_list.copy()\n","maxN=492\n","max_dict={}\n","\n","for i in range(maxN):\n","    max_index = dist_list2.index(max(dist_list2))\n","    max_value = dist_list2.pop(max_index)\n","    dist_list2.insert(max_index,0)\n","    max_dict[max_index]=max_value\n","\n","actual =set(val[val['Class']==1].index)\n","predicted = set(max_dict.keys())\n","\n","print('실제로 비정상인 데이터 수:',len(actual))\n","print('실제 비정상인데 비정상으로 예측한 데이터 수:',len(actual.intersection(predicted)))\n","print('precision={}'.format(len(actual.intersection(predicted))/len(actual)))\n"],"metadata":{"id":"vgnhUX193-m4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.preprocessing import MinMaxScaler,StandardScaler\n","from tensorflow.keras import Model, Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.losses import MeanSquaredLogarithmicError\n","\n","train = pd.read_csv(\"open/train.csv\")\n","val = pd.read_csv(\"open/val.csv\")\n","\n","train.drop(columns = {\"ID\"},inplace=True)\n","val.drop(columns = {\"ID\"},inplace=True)\n","\n","tf.enable_eager_execution()\n","\n","\n","\n","class AutoEncoder(Model):\n","    def __init__(self, output_units, code_size=8):\n","        super().__init__()\n","        self.encoder = Sequential([\n","          Dense(64, activation='relu'),\n","          Dropout(0.1),\n","          Dense(32, activation='relu'),\n","          Dropout(0.1),\n","          Dense(16, activation='relu'),\n","          Dropout(0.1),\n","          Dense(code_size, activation='relu')\n","        ])\n","        self.decoder = Sequential([\n","          Dense(16, activation='relu'),\n","          Dropout(0.1),\n","          Dense(32, activation='relu'), ㅇ\n","          Dropout(0.1),\n","          Dense(64, activation='relu'),\n","          Dropout(0.1),\n","          Dense(output_units, activation='sigmoid')\n","        ])\n","\n","    def call(self, inputs):\n","        encoded = self.encoder(inputs)\n","        decoded = self.decoder(encoded)\n","        return decoded\n","  \n","\n","model = AutoEncoder(output_units=train.shape[1])\n","model.compile(loss='msle', metrics=['mse'], optimizer='adam')\n","\n","std_scaler = StandardScaler()\n","std_scaler.fit(train)\n","x_train = std_scaler.transform(train)\n","x_test = std_scaler.transform(val[val.columns[:-1]])\n","\n","history = model.fit(\n","    x_train,\n","    x_train,\n","    epochs=20,\n","    batch_size=512,\n","    validation_data=(x_test, x_test)\n",")\n","\n","def find_threshold(model, x_train_scaled):\n","    reconstructions = model.predict(x_train_scaled)\n","      # provides losses of individual instances\n","    reconstruction_errors = tf.keras.losses.msle(reconstructions, x_train_scaled)\n","      # threshold for anomaly scores\n","    threshold = np.mean(reconstruction_errors.numpy()) \\\n","+ np.std(reconstruction_errors.numpy())\n","    return threshold\n","\n","def get_predictions(model, x_test_scaled, threshold):\n","    predictions = model.predict(x_test_scaled)\n","      # provides losses of individual instances\n","    errors = tf.keras.losses.msle(predictions, x_test_scaled)\n","      # 0 = anomaly, 1 = normal\n","    anomaly_mask = pd.Series(errors) > threshold\n","    preds = anomaly_mask.map(lambda x: 1.0 if x == True else 0.0)\n","    \n","    return preds\n","\n","threshold = find_threshold(model, x_train)\n","print(f\"Threshold: {threshold}\")\n","\n","# Threshold: 0.01001314025746261\n","predictions = get_predictions(model, x_test, threshold)\n","\n","predictions.value_counts()\n","\n","from sklearn.metrics import classification_report\n","print(classification_report(val['Class'].values,predictions))\n","\n","predictions = get_predictions(model, x_test, threshold)"],"metadata":{"id":"D8nV1IzD6td2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = pd.read_csv(\"open/test.csv\")\n","\n","test.drop(columns = {\"ID\"},inplace=True)\n","scale_test = std_scaler.transform(test)\n","test['Class'] = get_predictions(model, scale_test, threshold)\n","\n","sample = pd.read_csv(\"open/sample_submission.csv\")\n","sample['Class'] = test['Class']\n","sample.to_csv(\"open/sample_submission.csv\",index=False)"],"metadata":{"id":"5uhj8c4H64zi"},"execution_count":null,"outputs":[]}]}